{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff509f7",
   "metadata": {},
   "source": [
    "# Notebook with GTFS methods\n",
    "\n",
    "Goals: \n",
    "\n",
    "* Make a way to calculate the scheduled number of current active trips given a date, time, and route. \n",
    "    - Take datetime and find what services are active on that date \n",
    "    - Find what trips run on those services + route \n",
    "    - Find which of those trips are \"in progress\" per stop_times\n",
    "* ~Output most common shape by route~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffac7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import requests\n",
    "import pendulum\n",
    "from io import BytesIO\n",
    "import shapely\n",
    "import geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a22d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"private\", will assume you have write permissions and allow you to write; else will not attempt to write files\n",
    "BUCKET_TYPE = \"public\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local \n",
    "# CTA_GTFS = zipfile.ZipFile('cta_gtfs_20220509.zip')\n",
    "# s3\n",
    "# follow https://pythonguides.com/download-zip-file-from-url-using-python/\n",
    "# CTA_GTFS = zipfile.ZipFile(BytesIO(requests.get('https://chn-ghost-buses-public.s3.us-east-2.amazonaws.com/cta_static_gtfs/cta_gtfs_20220509.zip').content))\n",
    "# cta website\n",
    "\n",
    "VERSION_ID = '20220718'\n",
    "\n",
    "CTA_GTFS = zipfile.ZipFile(BytesIO(requests.get(f'https://transitfeeds.com/p/chicago-transit-authority/165/{VERSION_ID}/download').content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60357a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTFSFeed:\n",
    "    def __init__(self, gtfs_zipfile):\n",
    "        self.gtfs_zipfile = gtfs_zipfile\n",
    "        try: \n",
    "            with self.gtfs_zipfile.open('stops.txt') as file:\n",
    "                    self.stops = pd.read_csv(file, dtype = 'object')\n",
    "                    print(\"stops.txt loaded\")\n",
    "            with self.gtfs_zipfile.open('stop_times.txt') as file:\n",
    "                    self.stop_times = pd.read_csv(file, dtype = 'object')\n",
    "                    print(\"stop_times.txt loaded\")\n",
    "            with self.gtfs_zipfile.open('routes.txt') as file:\n",
    "                    self.routes = pd.read_csv(file, dtype = 'object')\n",
    "                    print(\"routes.txt loaded\")\n",
    "            with self.gtfs_zipfile.open('trips.txt') as file:\n",
    "                    self.trips = pd.read_csv(file, dtype = 'object')\n",
    "                    print(\"trips.txt loaded\")\n",
    "        except KeyError as e:\n",
    "            print(\"GTFS is missing required file\")\n",
    "            print(e)\n",
    "        if 'calendar.txt' in self.gtfs_zipfile.namelist():\n",
    "                with self.gtfs_zipfile.open('calendar.txt') as file:\n",
    "                        self.calendar = pd.read_csv(file, dtype = 'object')\n",
    "                        print(\"calendar.txt loaded\")\n",
    "        else:\n",
    "            print(\"no calendar.txt found\")\n",
    "        if 'calendar_dates.txt' in self.gtfs_zipfile.namelist():\n",
    "                with self.gtfs_zipfile.open('calendar_dates.txt') as file:\n",
    "                        self.calendar_dates = pd.read_csv(file, dtype = 'object')\n",
    "                        print(\"calendar_dates.txt loaded\")\n",
    "        else:\n",
    "            print(\"no calendar_dates.txt found\")\n",
    "        if 'shapes.txt' in self.gtfs_zipfile.namelist():\n",
    "                with self.gtfs_zipfile.open('shapes.txt') as file:\n",
    "                        self.shapes = pd.read_csv(file, dtype = 'object')\n",
    "                        print(\"shapes.txt loaded\")\n",
    "        else:\n",
    "            print(\"no shapes.txt found\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GTFSFeed(CTA_GTFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c84c30",
   "metadata": {},
   "source": [
    "## Basic data transformations\n",
    "\n",
    "Ex. creating actual timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_timestamp(s, date):\n",
    "#     parts = s.split(':')\n",
    "#     assert len(parts)==3\n",
    "#     if int(parts[0]) > 23:\n",
    "#         num_parts = [int(parts[0]) - 24, int(parts[1]), int(parts[2])]\n",
    "#     else:\n",
    "#         num_parts = [int(parts[0]), int(parts[1]), int(parts[2])]\n",
    "#     return pendulum.datetime(year = date.year, month = date.month, day = date.day, hour = num_parts[0], minute = num_parts[1], second = num_parts[2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hour(s):\n",
    "    parts = s.split(':')\n",
    "    assert len(parts)==3\n",
    "    hour = int(parts[0])\n",
    "    if hour >= 24:\n",
    "        hour -= 24\n",
    "    return hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9217f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dates_hours(data):\n",
    "    # convert string dates to actual datetimes in calendar.txt and calendar_dates.txt\n",
    "    data.calendar['start_date_dt'] = data.calendar['start_date'].apply(lambda x: pendulum.from_format(x, 'YYYYMMDD'))\n",
    "    data.calendar['end_date_dt'] = data.calendar['end_date'].apply(lambda x: pendulum.from_format(x, 'YYYYMMDD'))\n",
    "    data.calendar_dates['date_dt'] = data.calendar_dates['date'].apply(lambda x: pendulum.from_format(x, 'YYYYMMDD'))\n",
    "    \n",
    "    # extract hour from stop_times timestamps \n",
    "    data.stop_times['arrival_hour'] = data.stop_times.arrival_time.apply(lambda x: get_hour(x))\n",
    "    data.stop_times['departure_hour'] = data.stop_times.departure_time.apply(lambda x: get_hour(x))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43200f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = format_dates_hours(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96410a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there are no dwell periods that cross hour boundary\n",
    "data.stop_times[data.stop_times.arrival_hour != data.stop_times.departure_hour]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trip_summary(data):\n",
    "    # construct a datetime index that has every day between calendar start and end \n",
    "    calendar_date_range = pd.DataFrame(pd.date_range(min(data.calendar.start_date_dt), max(data.calendar.end_date_dt)), columns = ['raw_date'])\n",
    "    \n",
    "    # cross join calendar index with actual calendar to get all combos of possible dates & services \n",
    "    calendar_cross = calendar_date_range.merge(data.calendar, how = \"cross\")\n",
    "    \n",
    "    # extract day of week from date index date\n",
    "    calendar_cross['dayofweek'] = calendar_cross['raw_date'].dt.dayofweek\n",
    "    \n",
    "    # take wide calendar data (one col per day of week) and make it long (one row per day of week)\n",
    "    actual_service = calendar_cross.melt(id_vars = ['raw_date', 'start_date_dt', 'end_date_dt', 'start_date', 'end_date', 'service_id', 'dayofweek'], var_name = 'cal_dayofweek', value_name = 'cal_val')\n",
    "    \n",
    "    # map the calendar input strings to day of week integers to align w pandas dayofweek output\n",
    "    actual_service['cal_daynum'] = actual_service['cal_dayofweek'].map({\n",
    "        'monday': 0,\n",
    "        'tuesday': 1,\n",
    "        'wednesday': 2,\n",
    "        'thursday': 3,\n",
    "        'friday': 4,\n",
    "        'saturday': 5,\n",
    "        'sunday': 6\n",
    "    })\n",
    "    \n",
    "    # now check for rows that \"work\"\n",
    "    # i.e., the day of week matches between datetime index & calendar input\n",
    "    # and the datetime index is between the calendar row's start and end dates\n",
    "    actual_service = actual_service[(actual_service.dayofweek == actual_service.cal_daynum) & \n",
    "                                   (actual_service.start_date_dt <= actual_service.raw_date) &\n",
    "                                   (actual_service.end_date_dt >= actual_service.raw_date)]\n",
    "    \n",
    "    # now merge in calendar dates to the datetime index to get overrides\n",
    "    actual_service = actual_service.merge(data.calendar_dates, how = 'outer', left_on = ['raw_date', 'service_id'], right_on = ['date_dt', 'service_id'])\n",
    "    \n",
    "    # now add a service happened flag for dates where the schedule indicates that this service occurred\n",
    "    # i.e.: calendar has a service indicator of 1 and there's no exception type from calendar_dates\n",
    "    # OR calendar_dates has exception type of 1\n",
    "    # otherwise no service \n",
    "    # https://stackoverflow.com/questions/21415661/logical-operators-for-boolean-indexing-in-pandas\n",
    "    actual_service['service_happened'] = ((actual_service['cal_val'] == '1') & \n",
    "                                          actual_service['exception_type'].isnull()) | (actual_service['exception_type'] == '1')\n",
    "\n",
    "    \n",
    "    # now fill in rows where calendar_dates had a date outside the bounds of the datetime index, so raw_date is always populated\n",
    "    actual_service['raw_date'] = actual_service['raw_date'].fillna(actual_service['date_dt'])\n",
    "    \n",
    "    # filter to only rows where service occurred\n",
    "    service_happened = actual_service[actual_service.service_happened]\n",
    "    \n",
    "    # join trips to only service that occurred\n",
    "    trips_happened = data.trips.merge(service_happened, how = 'left', on = 'service_id')\n",
    "    \n",
    "    # get only the trip / hour combos that actually occurred\n",
    "    trip_stop_hours = data.stop_times[['trip_id', 'arrival_hour']].drop_duplicates()\n",
    "    \n",
    "    # now join\n",
    "    # result has one row per date + row from trips.txt (incl. route) + hour\n",
    "    trip_summary = trips_happened.merge(trip_stop_hours, how = \"left\", on = \"trip_id\")\n",
    "    \n",
    "    return trip_summary\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fce10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_summary = make_trip_summary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb947dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_save(trip_summary): \n",
    "    # now group to get trips by hour by date by route\n",
    "    route_daily_hourly_summary = trip_summary.groupby(by = ['raw_date', 'route_id', 'arrival_hour'])['trip_id'].count().reset_index()\n",
    "\n",
    "    route_daily_hourly_summary.rename(columns = {'arrival_hour': 'hour', 'trip_id': 'trip_count', 'raw_date': 'date'}, inplace = True)\n",
    "    route_daily_hourly_summary.date = route_daily_hourly_summary.date.dt.date\n",
    "    if BUCKET_TYPE == \"private\"\n",
    "        route_daily_hourly_summary.to_csv(f's3://chn-ghost-buses-{BUCKET_TYPE}/schedule_summaries/route_level/schedule_route_daily_hourly_summary_v{VERSION_ID}.csv', index = False)\n",
    "    \n",
    "    # now group to get trips by hour by date by route by *direction*\n",
    "    route_dir_daily_hourly_summary = trip_summary.groupby(by = ['raw_date', 'route_id', 'direction', 'arrival_hour'])['trip_id'].count().reset_index()\n",
    "\n",
    "    route_dir_daily_hourly_summary.rename(columns = {'arrival_hour': 'hour', 'trip_id': 'trip_count', 'raw_date': 'date'}, inplace = True)\n",
    "    route_dir_daily_hourly_summary.date = route_dir_daily_hourly_summary.date.dt.date\n",
    "    if BUCKET_TYPE == \"private\":\n",
    "        route_dir_daily_hourly_summary.to_csv(f's3://chn-ghost-buses-{BUCKET_TYPE}/schedule_summaries/route_dir_level/schedule_route_dir_daily_hourly_summary_v{VERSION_ID}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_and_save(trip_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfd3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "221fd612",
   "metadata": {},
   "source": [
    "## Most common shape by route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trip count by route, direction, shape id\n",
    "trips_by_rte_direction = data.trips.groupby(['route_id', 'shape_id', 'direction'])['trip_id'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only most common shape id by route, direction\n",
    "# follow: https://stackoverflow.com/a/54041328\n",
    "most_common_shapes = trips_by_rte_direction.sort_values('trip_id').drop_duplicates(['route_id','direction'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get additional route attributes\n",
    "most_common_shapes = most_common_shapes.merge(data.routes, how = 'left', on = 'route_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cfd2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make shapely points\n",
    "# https://www.geeksforgeeks.org/apply-function-to-every-row-in-a-pandas-dataframe/\n",
    "data.shapes['pt'] = data.shapes.apply(lambda row: shapely.geometry.Point((float(row['shape_pt_lon']), float(row['shape_pt_lat']))), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shapes['shape_pt_sequence'] = pd.to_numeric(data.shapes['shape_pt_sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct sorted list of shapely points\n",
    "# custom aggregation function: https://stackoverflow.com/a/10964938\n",
    "\n",
    "def make_linestring_of_points(sub_df):\n",
    "    sorted_df = sub_df.sort_values(by = 'shape_pt_sequence')\n",
    "    return shapely.geometry.LineString(list(sorted_df['pt']))\n",
    "\n",
    "constructed_shapes = data.shapes.groupby('shape_id').apply(make_linestring_of_points).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3672e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in the other route attributes\n",
    "final = most_common_shapes.merge(constructed_shapes, how = 'left', on = 'shape_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a \"geometry\" column for geopandas\n",
    "final['geometry'] = final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the geopandas geodataframe\n",
    "final_gdf = geopandas.GeoDataFrame(data = final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column that's a list of shapely points\n",
    "final_gdf = final_gdf.drop(0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a09727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gis.stackexchange.com/questions/11910/meaning-of-simplifys-tolerance-parameter\n",
    "final_gdf['geometry'] = final_gdf['geometry'].simplify(.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file as geojson (this saves locally)\n",
    "with open('route_shapes_simplified_linestring.geojson', 'w') as f:\n",
    "    f.write(final_gdf.loc[(final_gdf['route_type'] == '3')].to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e4fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_conda",
   "language": "python",
   "name": "base_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
